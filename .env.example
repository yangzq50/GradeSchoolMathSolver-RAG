# AI Model Service (Legacy - for backward compatibility)
AI_MODEL_URL=http://localhost:12434
AI_MODEL_NAME=ai/llama3.2:1B-Q4_0
LLM_ENGINE=llama.cpp

# Embedding Service (Legacy - for backward compatibility)
EMBEDDING_MODEL_URL=http://localhost:12434
EMBEDDING_MODEL_NAME=ai/embeddinggemma:300M-Q8_0

# New Configurable Model Endpoints (Recommended)
# Full URLs for model services - override these to customize endpoints
# If not set, they default to: {AI_MODEL_URL}/engines/{LLM_ENGINE}/v1/{endpoint}
GENERATION_SERVICE_URL=http://localhost:12434/engines/llama.cpp/v1/chat/completions
GENERATION_MODEL_NAME=ai/llama3.2:1B-Q4_0
EMBEDDING_SERVICE_URL=http://localhost:12434/engines/llama.cpp/v1/embeddings

# Database Backend Selection
# Options: elasticsearch, mariadb
DATABASE_BACKEND=mariadb

# Elasticsearch Configuration
ELASTICSEARCH_HOST=localhost
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_INDEX=quiz_history

# MariaDB Configuration (when DATABASE_BACKEND=mariadb)
MARIADB_HOST=localhost
MARIADB_PORT=3306
MARIADB_USER=math_solver
MARIADB_PASSWORD=math_solver_password
MARIADB_DATABASE=math_solver

# Web UI
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=False

# Teacher Service (optional feature for wrong answer feedback)
TEACHER_SERVICE_ENABLED=True
