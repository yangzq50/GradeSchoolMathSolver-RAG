# AI Model Service (Legacy - for backward compatibility)
AI_MODEL_URL=http://localhost:12434
AI_MODEL_NAME=ai/llama3.2:1B-Q4_0
LLM_ENGINE=llama.cpp

# Embedding Service (Legacy - for backward compatibility)
EMBEDDING_MODEL_URL=http://localhost:12434
EMBEDDING_MODEL_NAME=ai/embeddinggemma:300M-Q8_0

# New Configurable Model Endpoints (Recommended)
# Full URLs for model services - override these to customize endpoints
# If not set, they default to: {AI_MODEL_URL}/engines/{LLM_ENGINE}/v1/{endpoint}
GENERATION_SERVICE_URL=http://localhost:12434/engines/llama.cpp/v1/chat/completions
GENERATION_MODEL_NAME=ai/llama3.2:1B-Q4_0
EMBEDDING_SERVICE_URL=http://localhost:12434/engines/llama.cpp/v1/embeddings

# Database Backend Selection
# Options: elasticsearch, mariadb
DATABASE_BACKEND=mariadb

# Elasticsearch Configuration
ELASTICSEARCH_HOST=localhost
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_INDEX=quiz_history

# MariaDB Configuration (when DATABASE_BACKEND=mariadb)
MARIADB_HOST=localhost
MARIADB_PORT=3306
MARIADB_USER=math_solver
MARIADB_PASSWORD=math_solver_password
MARIADB_DATABASE=math_solver

# Web UI
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=False

# Teacher Service (optional feature for wrong answer feedback)
TEACHER_SERVICE_ENABLED=True

# Embedding Storage Configuration (for RAG features)
# Number of embedding columns to store per record
EMBEDDING_COLUMN_COUNT=2
# Dimension of each embedding column (768 for EmbeddingGemma)
# Can be comma-separated for different dimensions per column
EMBEDDING_DIMENSIONS=768
# Embedding column names (comma-separated)
EMBEDDING_COLUMN_NAMES=question_embedding,equation_embedding
# Source text columns for embedding generation (comma-separated)
# Each source column corresponds to an embedding column at the same index
EMBEDDING_SOURCE_COLUMNS=question,equation
# Elasticsearch vector similarity metric: cosine, dot_product, l2_norm
ELASTICSEARCH_VECTOR_SIMILARITY=cosine
